{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBL Process Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utils.LogFile import LogFile\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two different methods: \n",
    "- One csv file, which still has to be split into training and test data\n",
    "- Two csv files, which are already split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute columns here\n",
    "case_attr = \"case concept:name\"\n",
    "act_attr = \"event concept:name\"\n",
    "time_attr = \"event time:timestamp\"\n",
    "path = \"Data/sub_dataset.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"Data/sub_dataset.csv\"\n",
    "#baseline_log = LogFile(path, \",\", 0, None, time_attr='event time:timestamp', trace_attr=case_attr,\n",
    "#                    activity_attr=act_attr, convert=False, k=3)\n",
    "\n",
    "#train_base_log, test_base_log = baseline_log.splitTrainTest(65, split_case=False, method=\"test-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"Data/BPI_Challenge_2012-test.csv\"\n",
    "\n",
    "#baseline_log = LogFile(path, \",\", 0, None, time_attr='event time:timestamp', trace_attr=case_attr,\n",
    "#                    activity_attr=act_attr, convert=False, k=3)\n",
    "\n",
    "#train_base_log, test_base_log = baseline_log.splitTrainTest(70, split_case=False, method=\"test-train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"Data/sub_data_train.csv\" \n",
    "path_test = \"Data/sub_data_test.csv\"\n",
    "\n",
    "#path_train = 'Data/BPI_Challenge_2012-training.csv'\n",
    "#path_test = 'Data\\BPI_Challenge_2012-test.csv'\n",
    "\n",
    "train_base_log = LogFile(path_train, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                   activity_attr=act_attr, convert=False, k=3)\n",
    "test_base_log = LogFile(path_test, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(dataset):\n",
    "    \"\"\"Adds a new column to a dataset with the converted timestamp to datetime\"\"\"\n",
    "\n",
    "    date_list = []\n",
    "\n",
    "    for time in dataset['event time:timestamp']:\n",
    "        datex = time[:-4]\n",
    "        date = datetime.strptime(datex, '%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "        date_list.append(date)\n",
    "\n",
    "    dataset['time and date'] = date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add actual next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_actual_next(df_case):\n",
    "    \"\"\"Adds the actual next activity and time to next event to the final dataframe\"\"\"\n",
    "\n",
    "\n",
    "    # Create a list for all the actual next events for an case\n",
    "    event_lst = [event for event in df_case['event concept:name']] # Gets a list of all events for a specific trace\n",
    "    event_lst = event_lst[1:] # Erase the first activity from the list (thus the second activity becomes first in the list)\n",
    "    event_lst.append('-') # Append a '-' to the end of the list (the last activity does not have a next activity)\n",
    "    \n",
    "    # Create a list for time of the next event\n",
    "    nexttime_lst1 = [time for time in df_case['time and date']]\n",
    "    nexttime_lst = nexttime_lst1[1:]\n",
    "    nexttime_lst.append(nexttime_lst[-1])\n",
    "\n",
    "    # Create the time difference list\n",
    "    time_diff = []\n",
    "    for i in range(len(nexttime_lst)):\n",
    "        time_diff.append((nexttime_lst[i] - nexttime_lst1[i]).total_seconds())\n",
    "\n",
    "    # Append columns to the case dataframe\n",
    "    df_case['Next event'] = event_lst\n",
    "    df_case['Time to next event'] = time_diff\n",
    "\n",
    "    trace_len = len(df_case)\n",
    "\n",
    "    return trace_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_time(df_case, count_dict, time_dict):\n",
    "    for index, row in df_case.iterrows():\n",
    "        \n",
    "        # Get the amount of times an action occured in a certain position {action : {position_1 : count_1, position_2: count_2}}\n",
    "        if row['event concept:name'] in count_dict:\n",
    "            if index in count_dict[row['event concept:name']]:\n",
    "                count_dict[row['event concept:name']][index] += 1\n",
    "            else:\n",
    "                count_dict[row['event concept:name']].update({index: 1})\n",
    "        else:\n",
    "            count_dict[row['event concept:name']] = {index: 1}\n",
    "        \n",
    "        # Summation of the times to next action per position (index) {position: {\"sum\": summation_of_time, \"count\": amount_of_times_occured (to calculate mean)}}\n",
    "        if index in time_dict:\n",
    "            time_dict[index]['sum'] += row['Time to next event']\n",
    "            time_dict[index]['count'] += 1\n",
    "        else:\n",
    "            time_dict[index] = {'sum': row['Time to next event'], 'count': 1}\n",
    "\n",
    "def get_position_rank(max_trace_len, count_dict):\n",
    "    pos_rank_dict = {}\n",
    "    for i in range(max_trace_len):\n",
    "        init = 0\n",
    "        task = 0\n",
    "        for key in count_dict.keys():\n",
    "            try:\n",
    "                new = count_dict[key][i]\n",
    "            except:\n",
    "                new = 0\n",
    "            if new > init:\n",
    "                init = new\n",
    "                task = key\n",
    "\n",
    "        pos_rank_dict.update({i: task})\n",
    "    \n",
    "    return pos_rank_dict\n",
    "\n",
    "def get_mean_time(total_time_dict):\n",
    "    mean_time_dict = {}\n",
    "    for position in total_time_dict.keys():\n",
    "        mean_time = total_time_dict[position]['sum'] / total_time_dict[position]['count']\n",
    "        mean_time_dict[position] = mean_time\n",
    "    \n",
    "    return mean_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_pred(df_case, pos_rank_dict, mean_time_dict):\n",
    "    \n",
    "    # Prediction for the action\n",
    "    pred_act_lst = [pos_rank_dict[i] for i in range(len(df_case))]\n",
    "    pred_act_lst = pred_act_lst[1:]\n",
    "    pred_act_lst.append('-')\n",
    "\n",
    "    # Prediction for time\n",
    "    pred_time_lst = [mean_time_dict[i] for i in range(len(df_case))]\n",
    "\n",
    "    df_case['Event prediction'] = pred_act_lst \n",
    "    df_case['Time prediction'] = pred_time_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(dataframe, maximum=None):\n",
    "    \"\"\"Returns the training dataset with predictions and 2 dictionaries which predict next action and nexttime based on position\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_actual = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Creating a dataframe with the actual events\n",
    "\n",
    "    cases = list(dataset['case concept:name'].unique())  \n",
    "    max_trace_len = 0  \n",
    "    pos_count_dict = {}\n",
    "    time_dict = {}\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = dataset[dataset['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        trace_len = add_actual_next(df_case)\n",
    "        get_position_time(df_case, pos_count_dict, time_dict)\n",
    "        df_actual = pd.concat([df_actual, df_case])\n",
    "\n",
    "        if trace_len > max_trace_len:\n",
    "            max_trace_len = trace_len\n",
    "    \n",
    "\n",
    "\n",
    "    # Creating the predicitions\n",
    "    df_predicted = pd.DataFrame()\n",
    "    \n",
    "    pos_rank_dict = get_position_rank(max_trace_len, pos_count_dict)\n",
    "    mean_time_dict = get_mean_time(time_dict)\n",
    "\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = df_actual[df_actual['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        create_event_pred(df_case, pos_rank_dict, mean_time_dict)\n",
    "        df_predicted = pd.concat([df_predicted,df_case])\n",
    "\n",
    "\n",
    "\n",
    "    return df_predicted, pos_rank_dict, mean_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(dataframe, train_pos, train_time):\n",
    "    \"\"\"Creates the test dataset including the predictions based on the training dataset\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_predict = pd.DataFrame()\n",
    "    cases = list(dataset['case concept:name'].unique())  \n",
    "    for case in cases:\n",
    "        df_case = dataset[dataset['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        _ = add_actual_next(df_case)\n",
    "        create_event_pred(df_case, train_pos, train_time)\n",
    "        df_predict = pd.concat([df_predict, df_case])\n",
    "    \n",
    "    return df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataset):\n",
    "    event_accuracy = np.mean(dataset['Next event'] ==  dataset['Event prediction'])\n",
    "    time_accuracy = np.mean(abs(dataset['Time to next event'] - dataset['Time prediction'])) / 86400  # Mean Absolute Error in days\n",
    "    \n",
    "    return event_accuracy, time_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_base_log.get_data()\n",
    "test_df = test_base_log.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_pos, train_time = train_baseline(train_df)\n",
    "test_df = test_baseline(test_df, train_pos, train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47865303668069753 0.6874050509292106\n"
     ]
    }
   ],
   "source": [
    "train_event_acc, train_time_acc = get_accuracy(train_df)\n",
    "test_event_acc, test_time_acc = get_accuracy(test_df)\n",
    "\n",
    "print(test_event_acc, test_time_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(df,\n",
    "                  target_column, target_column2, target_column3,\n",
    "                  target_result, target_result2, target_result3\n",
    "                 ):\n",
    "    \"\"\"Add column to df with integers for the target.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    \n",
    "    targets2 = df_mod[target_column3].unique()\n",
    "    map_to_int2 = {name: n for n, name in enumerate(targets2)}\n",
    "    \n",
    "    \n",
    "    df_mod[f\"{target_result}\"] = df_mod[target_column].replace(map_to_int)\n",
    "    df_mod[f\"{target_result2}\"] = df_mod[target_column2].replace(map_to_int)\n",
    "    df_mod[f\"{target_result3}\"] = df_mod[target_column3].replace(map_to_int2)\n",
    "\n",
    "    return (df_mod)\n",
    "\n",
    "train_df = encode_target(train_df,\n",
    "                                           \"event concept:name\", \"Next event\", \"event lifecycle:transition\",\n",
    "                                           \"current state\", \"next state\", \"lifecycle\")\n",
    "train_df['next state'].replace('-', None, inplace=True)\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "test_df = encode_target(test_df,\n",
    "                                           \"event concept:name\", \"Next event\", \"event lifecycle:transition\",\n",
    "                                           \"current state\", \"next state\", \"lifecycle\")\n",
    "test_df['next state'].replace('-', None, inplace=True)\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree event prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24461815995189418"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sum = 0\n",
    "test_sum = 0\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    y = train_df['next state'].astype(int)\n",
    "    X = train_df[['current state', 'lifecycle']].astype(int)\n",
    "    clf = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')\n",
    "    clf = clf.fit(X, y)\n",
    "    \n",
    "    train_df['tree prediction'] = clf.predict(train_df[['current state', 'lifecycle']])\n",
    "    test_df['tree prediction'] = clf.predict(test_df[['current state', 'lifecycle']])\n",
    "    \n",
    "    correct_event = 0 \n",
    "    total = 0\n",
    "    for index, row in test_df.iterrows():\n",
    "        total += 1\n",
    "        if row['next state'] == row['tree prediction']:\n",
    "            correct_event += 1\n",
    "        \n",
    "    accuracy_event = correct_event/total \n",
    "    test_sum += accuracy_event\n",
    "\n",
    "test_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4153229563372753"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sum2 = 0\n",
    "test_sum2 = 0\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    y2 = train_df['Time to next event']\n",
    "    X2 = train_df[['current state', 'lifecycle']].astype(int)\n",
    "    clf2 = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')\n",
    "    clf2 = clf2.fit(X2, y2)\n",
    "    \n",
    "    train_df['tree time prediction'] = clf2.predict(train_df[['current state', 'lifecycle']])\n",
    "    test_df['tree time prediction'] = clf2.predict(test_df[['current state', 'lifecycle']])\n",
    "    \n",
    "    correct_event = 0 \n",
    "    total = 0\n",
    "    for index, row in test_df.iterrows():\n",
    "        total += 1\n",
    "        correct_event += abs(row['Time to next event'] - row['tree time prediction'])\n",
    "        \n",
    "    test_sum2 += correct_event/total/86400\n",
    "    \n",
    "test_sum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_log(log):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    activities = np.unique(log.data[log.activity])\n",
    "    X = np.zeros((len(log.contextdata), log.k, len(activities)+ 1 + 4), dtype=np.float32)\n",
    "    y_a = np.zeros((len(log.contextdata), len(activities) + 2), dtype=np.float32)\n",
    "    y_t = np.zeros((len(log.contextdata)), dtype=np.float32)\n",
    "    j = 0\n",
    "    df = log.contextdata\n",
    "    time_diff = 0\n",
    "    for row in log.contextdata.iterrows():\n",
    "        \n",
    "            act = getattr(row[1], log.activity)\n",
    "            event_str = getattr(row[1], log.time)\n",
    "            prev_str = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "            #prev_1_str = getattr(row[1], \"%s_Prev1\" % (log.time))\n",
    "            event_time = time.strptime(event_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "            if prev_str != 0:\n",
    "                prev_time = time.strptime(prev_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                diff = diff_prev_event.total_seconds()\n",
    "            else: \n",
    "                diff = 0\n",
    "            y_a[j, act] = 1\n",
    "            y_t[j] = diff            \n",
    "\n",
    "            k = 0\n",
    "            \n",
    "            for i in range(log.k -1, -1, -1):\n",
    "                \n",
    "                if getattr(row[1], \"%s_Prev%i\" % (log.activity, i)) != 0: # 0 indicates no activity (first activity is encoded to 1)\n",
    "                    X[j, log.k - i - 1, getattr(row[1], \"%s_Prev%i\" % (log.activity, i))] = 1\n",
    "                X[j, log.k - i - 1, len(activities)+1] = k\n",
    "                X[j, log.k - i - 1, len(activities) + 2] = time_diff # Diff in seconds\n",
    "\n",
    " \n",
    "                str_time = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "                if str_time != 0:\n",
    "                    event_time = time.strptime(str_time, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                    X[j, log.k - i - 1, len(activities) + 3] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_wday  # Day of the week\n",
    "    \n",
    "                try:\n",
    "                    prev_str = getattr(row[1], \"%s_Prev1\" % (log.time))\n",
    "                    #print(\"First success!\", prev_str)\n",
    "                    if prev_str != 0:\n",
    "\n",
    "                        prev_time = time.strptime(prev_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                        diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                        time_diff = diff_prev_event.total_seconds()\n",
    "                        #print(time_diff)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "\n",
    "                #if str_time != 0: #No event\n",
    "                #    try:\n",
    "                #        event_time = time.strptime(str_time, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                #    except ValueError:\n",
    "                #        event_time = time.strptime(str_time, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "\n",
    "\n",
    "                #    if prev_str != 0:\n",
    "                #        try:\n",
    "                #           prev_time = time.strptime(prev_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                #        except ValueError:\n",
    "    \n",
    "                #            prev_time = time.strptime(prev_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                #        \n",
    "                #        diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                #                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                #        diff = 86400 * diff_prev_event.days + diff_prev_event.seconds\n",
    "                        #sum_duration += diff\n",
    "                        #count_duration += 1\n",
    "                        \n",
    "                    X[j, log.k - i - 1, len(activities) + 3] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_wday  # Day of the week\n",
    "\n",
    "                k += 1\n",
    "\n",
    "            j += 1\n",
    "\n",
    "    return X, y_a, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(log, epochs=4, early_stop=42):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "    from tensorflow.keras.layers import Input\n",
    "    from tensorflow.keras.layers import Dense, BatchNormalization, LSTM\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "    print(\"Transforming log...\")\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "\n",
    "    # build the model:\n",
    "    print('Build model...')\n",
    "    main_input = Input(shape=(log.k, len(np.unique(log.data[log.activity]))+5), name='main_input')\n",
    "    # train a 2-layer LSTM with one shared layer\n",
    "    l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer\n",
    "    b1 = BatchNormalization()(l1)\n",
    "    l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction\n",
    "    b2_1 = BatchNormalization()(l2_1)\n",
    "    l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in time prediction\n",
    "    b2_2 = BatchNormalization()(l2_2)\n",
    "\n",
    "    act_output = Dense(len(np.unique(log.data[log.activity])) + 2, activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\n",
    "    time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "    opt = Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "    model.compile(loss={'act_output':'categorical_crossentropy', 'time_output': 'mae'}, optimizer=opt)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop)\n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(\"model\", 'model_{epoch:03d}-{val_loss:.2f}.h5'), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    if len(y_a) > 10:\n",
    "        split = 0.2\n",
    "    else:\n",
    "        split = 0\n",
    "\n",
    "    model.fit(X, {'act_output': y_a, 'time_output': y_t}, validation_split=split, verbose=2, callbacks=[early_stopping, lr_reducer], batch_size=log.k, epochs=epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, log):\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "    pred_act, pred_time = model.predict(X)\n",
    "    predict_vals = np.argmax(pred_act, axis=1)\n",
    "    pred_time = pred_time.reshape(-1)\n",
    "    #predict_probs = predictions[np.arange(predictions.shape[0]), predict_vals]\n",
    "    expected_vals = np.argmax(y_a, axis=1)\n",
    "    #expected_probs = predictions[np.arange(predictions.shape[0]), expected_vals]\n",
    "    activity_acc = np.mean(expected_vals ==  predict_vals)\n",
    "    mae_time = np.mean(abs(y_t - pred_time)) / 86400\n",
    "    return predict_vals, pred_time, activity_acc, mae_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create k-context: 2\n",
      "Create k-context: 2\n",
      "Created k context\n"
     ]
    }
   ],
   "source": [
    "LSTM_log_train = LogFile(path_train, \",\", 0, None, 'event time:timestamp', case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=2)\n",
    "LSTM_log_test = LogFile(path_test, \",\", 0, None, 'event time:timestamp', case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=2)\n",
    "\n",
    "LSTM_map_train = LSTM_log_train.int_convert()\n",
    "LSTM_map_test = LSTM_log_test.int_convert()\n",
    "\n",
    "LSTM_log_train.remove_attributes(['eventID', 'case REG_DATE', 'case AMOUNT_REQ', 'event lifecycle:transition'])\n",
    "LSTM_log_test.remove_attributes(['eventID', 'case REG_DATE', 'case AMOUNT_REQ', 'event lifecycle:transition'])\n",
    "\n",
    "LSTM_log_train.create_k_context()\n",
    "LSTM_log_test.create_k_context()\n",
    "\n",
    "\n",
    "print(\"Created k context\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming log...\n",
      "Build model...\n",
      "Epoch 1/5\n",
      "5866/5866 - 275s - loss: 39024.9883 - act_output_loss: 2.5032 - time_output_loss: 39022.4297 - val_loss: 40667.0859 - val_act_output_loss: 2.3110 - val_time_output_loss: 40664.7656 - lr: 0.0020 - 275s/epoch - 47ms/step\n",
      "Epoch 2/5\n",
      "5866/5866 - 300s - loss: 39020.6367 - act_output_loss: 2.3324 - time_output_loss: 39018.2539 - val_loss: 40663.9492 - val_act_output_loss: 2.1426 - val_time_output_loss: 40661.8477 - lr: 0.0020 - 300s/epoch - 51ms/step\n",
      "Epoch 3/5\n"
     ]
    }
   ],
   "source": [
    "model = train_LSTM(LSTM_log_train, epochs=5, early_stop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_act, pred_time, acc_act, mae_time = test(model, LSTM_log_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2744630071599045 0.45543533890335647\n"
     ]
    }
   ],
   "source": [
    "print(acc_act, mae_time)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fafe8da2469ce22b82e1babda22c546ed40097b8f4cbd3e26d6446f22e5d370"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
