{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBL Process Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utils.LogFile import LogFile\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two different methods: \n",
    "- One csv file, which still has to be split into training and test data\n",
    "- Two csv files, which are already split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute columns here\n",
    "case_attr = \"Case ID\"\n",
    "act_attr = \"concept:name\"\n",
    "time_attr = \"Complete Timestamp\"\n",
    "path = \"data/BPI_Challenge_2012_end.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"Data/sub_dataset.csv\"\n",
    "#baseline_log = LogFile(path, \",\", 0, None, time_attr='event time:timestamp', trace_attr=case_attr,\n",
    "#                    activity_attr=act_attr, convert=False, k=3)\n",
    "\n",
    "#train_base_log, test_base_log = baseline_log.splitTrainTest(65, split_case=False, method=\"test-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"Data/BPI_Challenge_2012-test.csv\"\n",
    "\n",
    "#baseline_log = LogFile(path, \",\", 0, None, time_attr='event time:timestamp', trace_attr=case_attr,\n",
    "#                    activity_attr=act_attr, convert=False, k=3)\n",
    "\n",
    "#train_base_log, test_base_log = baseline_log.splitTrainTest(70, split_case=False, method=\"test-train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_train = \"Data/sub_data_train.csv\" \n",
    "#path_test = \"Data/sub_data_test.csv\"\n",
    "\n",
    "#path_train = 'Data/BPI_Challenge_2012-training.csv'\n",
    "#path_test = 'Data\\BPI_Challenge_2012-test.csv'\n",
    "\n",
    "train_base_log = LogFile(path_train, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                   activity_attr=act_attr, convert=False, k=3)\n",
    "test_base_log = LogFile(path_test, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(df_train, df_test):\n",
    "    # Standard Scaler \n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(df_train)\n",
    "    \n",
    "    # Scalar transformation\n",
    "    df_train_transform = scalar.transform(df_train)\n",
    "    df_test_transform = scalar.transform(df_test)\n",
    "\n",
    "    # PCA\n",
    "    df_train_transform = pca.transform(df_train_transform)\n",
    "    df_test_transform = pca.transform(df_test_transform)\n",
    "\n",
    "    return df_train_transform, df_test_transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# only keep the starting and end point of a case \n",
    "def split_train_test(path, split_interval):\n",
    "    df = pd.read_csv(path)\n",
    "    data = df[(df['concept:name'] == 'A_SUBMITTED-COMPLETE') | (df['concept:name'] == 'End-End')]\n",
    "    data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n",
    "\n",
    "\n",
    "    \n",
    "    loss = len(data)\n",
    "    \n",
    "    for i in split_interval:\n",
    "        train, test = train_test_split(data['Case ID'].unique(), test_size=(100-i)/100, shuffle=False)\n",
    "        train_data = data[data['Case ID'].isin(train)]\n",
    "        test_data = data[data['Case ID'].isin(test)]\n",
    "        \n",
    "        overlap = train_data[train_data['Complete Timestamp'] > test_data['Complete Timestamp'].min()][['Case ID']]\n",
    "        \n",
    "        if len(overlap) < loss:\n",
    "            loss = len(overlap)\n",
    "            best_train = train_data[~train_data['Case ID'].isin(overlap['Case ID'].tolist())]\n",
    "            best_test = test_data\n",
    "    \n",
    "    \n",
    "        print('Train data lost due to overlap: ' + str(len(overlap)/len(train_data)))\n",
    "        return best_train, best_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Activity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\BPI\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\.conda\\envs\\BPI\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\BPI\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Activity'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\20204502\\OneDrive - TU Eindhoven\\Documents\\GitHub\\Process-Mining\\Process-Mining\\Deliv_Notebook.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000053?line=0'>1</a>\u001b[0m train, test \u001b[39m=\u001b[39m split_train_test(\u001b[39m'\u001b[39;49m\u001b[39mdata/BPI_2012_Converted.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mrange\u001b[39;49m(\u001b[39m67\u001b[39;49m, \u001b[39m73\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\20204502\\OneDrive - TU Eindhoven\\Documents\\GitHub\\Process-Mining\\Process-Mining\\Deliv_Notebook.ipynb Cell 16'\u001b[0m in \u001b[0;36msplit_train_test\u001b[1;34m(path, split_interval)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000051?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_train_test\u001b[39m(path, split_interval):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000051?line=3'>4</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000051?line=4'>5</a>\u001b[0m     data \u001b[39m=\u001b[39m df[(df[\u001b[39m'\u001b[39;49m\u001b[39mActivity\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mA_SUBMITTED-COMPLETE\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m|\u001b[39m (df[\u001b[39m'\u001b[39m\u001b[39mActivity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mEnd-End\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000051?line=5'>6</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mComplete Timestamp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(data[\u001b[39m'\u001b[39m\u001b[39mComplete Timestamp\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000051?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[1;32m~\\.conda\\envs\\BPI\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/frame.py?line=3502'>3503</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/frame.py?line=3503'>3504</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/frame.py?line=3504'>3505</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/frame.py?line=3505'>3506</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/frame.py?line=3506'>3507</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\.conda\\envs\\BPI\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/20204502/.conda/envs/BPI/lib/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Activity'"
     ]
    }
   ],
   "source": [
    "train, test = split_train_test('data/BPI_2012_Converted.csv', range(67, 73))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(path_train) \n",
    "train_img = scaler.transform(path_train)\n",
    "test_img = scaler.transform(path_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(.95)\n",
    "\n",
    "pca.fit(path_train)\n",
    "\n",
    "train_img = pca.transform(path_train)\n",
    "test_img = pca.transform(path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(dataset):\n",
    "    \"\"\"Adds a new column to a dataset with the converted timestamp to datetime\"\"\"\n",
    "\n",
    "    date_list = []\n",
    "\n",
    "    for time in dataset['event time:timestamp']:\n",
    "        datex = time[:-4]\n",
    "        date = datetime.strptime(datex, '%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "        date_list.append(date)\n",
    "\n",
    "    dataset['time and date'] = date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add actual next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_actual_next(df_case):\n",
    "    \"\"\"Adds the actual next activity and time to next event to the final dataframe\"\"\"\n",
    "\n",
    "\n",
    "    # Create a list for all the actual next events for an case\n",
    "    event_lst = [event for event in df_case['event concept:name']] # Gets a list of all events for a specific trace\n",
    "    event_lst = event_lst[1:] # Erase the first activity from the list (thus the second activity becomes first in the list)\n",
    "    event_lst.append('-') # Append a '-' to the end of the list (the last activity does not have a next activity)\n",
    "    \n",
    "    # Create a list for time of the next event\n",
    "    nexttime_lst1 = [time for time in df_case['time and date']]\n",
    "    nexttime_lst = nexttime_lst1[1:]\n",
    "    nexttime_lst.append(nexttime_lst[-1])\n",
    "\n",
    "    # Create the time difference list\n",
    "    time_diff = []\n",
    "    for i in range(len(nexttime_lst)):\n",
    "        time_diff.append((nexttime_lst[i] - nexttime_lst1[i]).total_seconds())\n",
    "\n",
    "    # Append columns to the case dataframe\n",
    "    df_case['Next event'] = event_lst\n",
    "    df_case['Time to next event'] = time_diff\n",
    "\n",
    "    trace_len = len(df_case)\n",
    "\n",
    "    return trace_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_time(df_case, count_dict, time_dict):\n",
    "    for index, row in df_case.iterrows():\n",
    "        \n",
    "        # Get the amount of times an action occured in a certain position {action : {position_1 : count_1, position_2: count_2}}\n",
    "        if row['event concept:name'] in count_dict:\n",
    "            if index in count_dict[row['event concept:name']]:\n",
    "                count_dict[row['event concept:name']][index] += 1\n",
    "            else:\n",
    "                count_dict[row['event concept:name']].update({index: 1})\n",
    "        else:\n",
    "            count_dict[row['event concept:name']] = {index: 1}\n",
    "        \n",
    "        # Summation of the times to next action per position (index) {position: {\"sum\": summation_of_time, \"count\": amount_of_times_occured (to calculate mean)}}\n",
    "        if index in time_dict:\n",
    "            time_dict[index]['sum'] += row['Time to next event']\n",
    "            time_dict[index]['count'] += 1\n",
    "        else:\n",
    "            time_dict[index] = {'sum': row['Time to next event'], 'count': 1}\n",
    "\n",
    "def get_position_rank(max_trace_len, count_dict):\n",
    "    pos_rank_dict = {}\n",
    "    for i in range(max_trace_len):\n",
    "        init = 0\n",
    "        task = 0\n",
    "        for key in count_dict.keys():\n",
    "            try:\n",
    "                new = count_dict[key][i]\n",
    "            except:\n",
    "                new = 0\n",
    "            if new > init:\n",
    "                init = new\n",
    "                task = key\n",
    "\n",
    "        pos_rank_dict.update({i: task})\n",
    "    \n",
    "    return pos_rank_dict\n",
    "\n",
    "def get_mean_time(total_time_dict):\n",
    "    mean_time_dict = {}\n",
    "    for position in total_time_dict.keys():\n",
    "        mean_time = total_time_dict[position]['sum'] / total_time_dict[position]['count']\n",
    "        mean_time_dict[position] = mean_time\n",
    "    \n",
    "    return mean_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_pred(df_case, pos_rank_dict, mean_time_dict):\n",
    "    \n",
    "    # Prediction for the action\n",
    "    pred_act_lst = [pos_rank_dict[i] for i in range(len(df_case))]\n",
    "    pred_act_lst = pred_act_lst[1:]\n",
    "    pred_act_lst.append('-')\n",
    "\n",
    "    # Prediction for time\n",
    "    pred_time_lst = [mean_time_dict[i] for i in range(len(df_case))]\n",
    "\n",
    "    df_case['Event prediction'] = pred_act_lst \n",
    "    df_case['Time prediction'] = pred_time_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(dataframe, maximum=None):\n",
    "    \"\"\"Returns the training dataset with predictions and 2 dictionaries which predict next action and nexttime based on position\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_actual = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Creating a dataframe with the actual events\n",
    "\n",
    "    cases = list(dataset['case concept:name'].unique())  \n",
    "    max_trace_len = 0  \n",
    "    pos_count_dict = {}\n",
    "    time_dict = {}\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = dataset[dataset['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        trace_len = add_actual_next(df_case)\n",
    "        get_position_time(df_case, pos_count_dict, time_dict)\n",
    "        df_actual = pd.concat([df_actual, df_case])\n",
    "\n",
    "        if trace_len > max_trace_len:\n",
    "            max_trace_len = trace_len\n",
    "    \n",
    "\n",
    "\n",
    "    # Creating the predicitions\n",
    "    df_predicted = pd.DataFrame()\n",
    "    \n",
    "    pos_rank_dict = get_position_rank(max_trace_len, pos_count_dict)\n",
    "    mean_time_dict = get_mean_time(time_dict)\n",
    "\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = df_actual[df_actual['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        create_event_pred(df_case, pos_rank_dict, mean_time_dict)\n",
    "        df_predicted = pd.concat([df_predicted,df_case])\n",
    "\n",
    "\n",
    "\n",
    "    return df_predicted, pos_rank_dict, mean_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(dataframe, train_pos, train_time):\n",
    "    \"\"\"Creates the test dataset including the predictions based on the training dataset\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_predict = pd.DataFrame()\n",
    "    cases = list(dataset['case concept:name'].unique())  \n",
    "    for case in cases:\n",
    "        df_case = dataset[dataset['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        _ = add_actual_next(df_case)\n",
    "        create_event_pred(df_case, train_pos, train_time)\n",
    "        df_predict = pd.concat([df_predict, df_case])\n",
    "    \n",
    "    return df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataset):\n",
    "    event_accuracy = np.mean(dataset['Next event'] ==  dataset['Event prediction'])\n",
    "    time_accuracy = np.mean(abs(dataset['Time to next event'] - dataset['Time prediction'])) / 86400  # Mean Absolute Error in days\n",
    "    \n",
    "    return event_accuracy, time_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_base_log.get_data()\n",
    "test_df = test_base_log.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_pos, train_time = train_baseline(train_df)\n",
    "test_df = test_baseline(test_df, train_pos, train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47865303668069753 0.6874050509292073\n"
     ]
    }
   ],
   "source": [
    "train_event_acc, train_time_acc = get_accuracy(train_df)\n",
    "test_event_acc, test_time_acc = get_accuracy(test_df)\n",
    "\n",
    "print(test_event_acc, test_time_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(df,\n",
    "                  target_column, target_column2, target_column3,\n",
    "                  target_result, target_result2, target_result3\n",
    "                 ):\n",
    "    \"\"\"Add column to df with integers for the target.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    \n",
    "    targets2 = df_mod[target_column3].unique()\n",
    "    map_to_int2 = {name: n for n, name in enumerate(targets2)}\n",
    "    \n",
    "    \n",
    "    df_mod[f\"{target_result}\"] = df_mod[target_column].replace(map_to_int)\n",
    "    df_mod[f\"{target_result2}\"] = df_mod[target_column2].replace(map_to_int)\n",
    "    df_mod[f\"{target_result3}\"] = df_mod[target_column3].replace(map_to_int2)\n",
    "\n",
    "    return (df_mod)\n",
    "\n",
    "train_df = encode_target(train_df,\n",
    "                                           \"event concept:name\", \"Next event\", \"event lifecycle:transition\",\n",
    "                                           \"current state\", \"next state\", \"lifecycle\")\n",
    "train_df['next state'].replace('-', None, inplace=True)\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "test_df = encode_target(test_df,\n",
    "                                           \"event concept:name\", \"Next event\", \"event lifecycle:transition\",\n",
    "                                           \"current state\", \"next state\", \"lifecycle\")\n",
    "test_df['next state'].replace('-', None, inplace=True)\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>case concept:name</th>\n",
       "      <th>case REG_DATE</th>\n",
       "      <th>case AMOUNT_REQ</th>\n",
       "      <th>event concept:name</th>\n",
       "      <th>event lifecycle:transition</th>\n",
       "      <th>event time:timestamp</th>\n",
       "      <th>time and date</th>\n",
       "      <th>Next event</th>\n",
       "      <th>Time to next event</th>\n",
       "      <th>Event prediction</th>\n",
       "      <th>Time prediction</th>\n",
       "      <th>current state</th>\n",
       "      <th>next state</th>\n",
       "      <th>lifecycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1571958030336</td>\n",
       "      <td>174839</td>\n",
       "      <td>2011-10-05T16:53:32.364+02:00</td>\n",
       "      <td>6000</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:53:32.364</td>\n",
       "      <td>2011-10-05 16:53:32</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>0.524229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1571958030337</td>\n",
       "      <td>174839</td>\n",
       "      <td>2011-10-05T16:53:32.364+02:00</td>\n",
       "      <td>6000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:53:32.501</td>\n",
       "      <td>2011-10-05 16:53:32</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>35.0</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>37.274596</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1571958030338</td>\n",
       "      <td>174839</td>\n",
       "      <td>2011-10-05T16:53:32.364+02:00</td>\n",
       "      <td>6000</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:54:07.710</td>\n",
       "      <td>2011-10-05 16:54:07</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>7269.049927</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1576252997632</td>\n",
       "      <td>174842</td>\n",
       "      <td>2011-10-05T16:54:58.973+02:00</td>\n",
       "      <td>5000</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:54:58.973</td>\n",
       "      <td>2011-10-05 16:54:58</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>0.524229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1576252997633</td>\n",
       "      <td>174842</td>\n",
       "      <td>2011-10-05T16:54:58.973+02:00</td>\n",
       "      <td>5000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:54:59.079</td>\n",
       "      <td>2011-10-05 16:54:59</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>35.0</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>37.274596</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4492535791617</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>14-10-2011 13:32:47.462</td>\n",
       "      <td>2011-10-14 13:32:47</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>36.0</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>37.274596</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4492535791618</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>14-10-2011 13:33:23.060</td>\n",
       "      <td>2011-10-14 13:33:23</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>3783.0</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>7269.049927</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4492535791619</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>START</td>\n",
       "      <td>14-10-2011 14:36:26.685</td>\n",
       "      <td>2011-10-14 14:36:26</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>356.0</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>9534.675422</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4492535791620</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>14-10-2011 14:42:22.707</td>\n",
       "      <td>2011-10-14 14:42:22</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>4.0</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>345.043152</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4492535791621</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>14-10-2011 14:42:26.046</td>\n",
       "      <td>2011-10-14 14:42:26</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>8289.707317</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14665 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         eventID   case concept:name                  case REG_DATE  \\\n",
       "0   1571958030336             174839  2011-10-05T16:53:32.364+02:00   \n",
       "1   1571958030337             174839  2011-10-05T16:53:32.364+02:00   \n",
       "2   1571958030338             174839  2011-10-05T16:53:32.364+02:00   \n",
       "0   1576252997632             174842  2011-10-05T16:54:58.973+02:00   \n",
       "1   1576252997633             174842  2011-10-05T16:54:58.973+02:00   \n",
       "..            ...                ...                            ...   \n",
       "1   4492535791617             176972  2011-10-14T13:32:47.208+02:00   \n",
       "2   4492535791618             176972  2011-10-14T13:32:47.208+02:00   \n",
       "3   4492535791619             176972  2011-10-14T13:32:47.208+02:00   \n",
       "4   4492535791620             176972  2011-10-14T13:32:47.208+02:00   \n",
       "5   4492535791621             176972  2011-10-14T13:32:47.208+02:00   \n",
       "\n",
       "    case AMOUNT_REQ  event concept:name event lifecycle:transition  \\\n",
       "0              6000         A_SUBMITTED                   COMPLETE   \n",
       "1              6000   A_PARTLYSUBMITTED                   COMPLETE   \n",
       "2              6000          A_DECLINED                   COMPLETE   \n",
       "0              5000         A_SUBMITTED                   COMPLETE   \n",
       "1              5000   A_PARTLYSUBMITTED                   COMPLETE   \n",
       "..              ...                 ...                        ...   \n",
       "1             25000   A_PARTLYSUBMITTED                   COMPLETE   \n",
       "2             25000  W_Afhandelen leads                   SCHEDULE   \n",
       "3             25000  W_Afhandelen leads                      START   \n",
       "4             25000          A_DECLINED                   COMPLETE   \n",
       "5             25000  W_Afhandelen leads                   COMPLETE   \n",
       "\n",
       "       event time:timestamp       time and date          Next event  \\\n",
       "0   05-10-2011 16:53:32.364 2011-10-05 16:53:32   A_PARTLYSUBMITTED   \n",
       "1   05-10-2011 16:53:32.501 2011-10-05 16:53:32          A_DECLINED   \n",
       "2   05-10-2011 16:54:07.710 2011-10-05 16:54:07                   -   \n",
       "0   05-10-2011 16:54:58.973 2011-10-05 16:54:58   A_PARTLYSUBMITTED   \n",
       "1   05-10-2011 16:54:59.079 2011-10-05 16:54:59          A_DECLINED   \n",
       "..                      ...                 ...                 ...   \n",
       "1   14-10-2011 13:32:47.462 2011-10-14 13:32:47  W_Afhandelen leads   \n",
       "2   14-10-2011 13:33:23.060 2011-10-14 13:33:23  W_Afhandelen leads   \n",
       "3   14-10-2011 14:36:26.685 2011-10-14 14:36:26          A_DECLINED   \n",
       "4   14-10-2011 14:42:22.707 2011-10-14 14:42:22  W_Afhandelen leads   \n",
       "5   14-10-2011 14:42:26.046 2011-10-14 14:42:26                   -   \n",
       "\n",
       "    Time to next event        Event prediction  Time prediction  \\\n",
       "0                  0.0       A_PARTLYSUBMITTED         0.524229   \n",
       "1                 35.0           A_PREACCEPTED        37.274596   \n",
       "2                  0.0                       -      7269.049927   \n",
       "0                  1.0       A_PARTLYSUBMITTED         0.524229   \n",
       "1                 35.0           A_PREACCEPTED        37.274596   \n",
       "..                 ...                     ...              ...   \n",
       "1                 36.0           A_PREACCEPTED        37.274596   \n",
       "2               3783.0  W_Completeren aanvraag      7269.049927   \n",
       "3                356.0  W_Completeren aanvraag      9534.675422   \n",
       "4                  4.0  W_Completeren aanvraag       345.043152   \n",
       "5                  0.0                       -      8289.707317   \n",
       "\n",
       "    current state next state  lifecycle  \n",
       "0               0          1          0  \n",
       "1               1          2          0  \n",
       "2               2          2          0  \n",
       "0               0          1          0  \n",
       "1               1          2          0  \n",
       "..            ...        ...        ...  \n",
       "1               1         20          0  \n",
       "2              20         20          1  \n",
       "3              20          2          2  \n",
       "4               2         20          0  \n",
       "5              20         20          0  \n",
       "\n",
       "[14665 rows x 15 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree event prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24461815995189418"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sum = 0\n",
    "test_sum = 0\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    y = train_df['next state'].astype(int)\n",
    "    X = train_df[['current state', 'lifecycle']].astype(int)\n",
    "    clf = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')\n",
    "    clf = clf.fit(X, y)\n",
    "    \n",
    "    train_df['tree prediction'] = clf.predict(train_df[['current state', 'lifecycle']])\n",
    "    test_df['tree prediction'] = clf.predict(test_df[['current state', 'lifecycle']])\n",
    "    \n",
    "    correct_event = 0 \n",
    "    total = 0\n",
    "    for index, row in test_df.iterrows():\n",
    "        total += 1\n",
    "        if row['next state'] == row['tree prediction']:\n",
    "            correct_event += 1\n",
    "        \n",
    "    accuracy_event = correct_event/total \n",
    "    test_sum += accuracy_event\n",
    "\n",
    "test_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4153229563372753"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sum2 = 0\n",
    "test_sum2 = 0\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    y2 = train_df['Time to next event']\n",
    "    X2 = train_df[['current state', 'lifecycle']].astype(int)\n",
    "    clf2 = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')\n",
    "    clf2 = clf2.fit(X2, y2)\n",
    "    \n",
    "    train_df['tree time prediction'] = clf2.predict(train_df[['current state', 'lifecycle']])\n",
    "    test_df['tree time prediction'] = clf2.predict(test_df[['current state', 'lifecycle']])\n",
    "    \n",
    "    correct_event = 0\n",
    "    total = 0\n",
    "    for index, row in test_df.iterrows():\n",
    "        total += 1\n",
    "        correct_event += abs(row['Time to next event'] - row['tree time prediction'])\n",
    "        \n",
    "    test_sum2 += correct_event / total / 86400\n",
    "    \n",
    "test_sum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_log(log):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    activities = np.unique(log.data[log.activity])\n",
    "    X = np.zeros((len(log.contextdata), log.k, len(activities)+ 7), dtype=np.float32)\n",
    "    y_a = np.zeros((len(log.contextdata), len(activities) + 1), dtype=np.float32)\n",
    "    y_t = np.zeros((len(log.contextdata)), dtype=np.float32)\n",
    "    j = 0\n",
    "    df = log.contextdata\n",
    "    events_this_day = 0\n",
    "    last_event_day = None\n",
    "    time_diff = 0\n",
    "    for row in log.contextdata.iterrows():\n",
    "        \n",
    "            act = getattr(row[1], log.activity)\n",
    "            event_str = getattr(row[1], log.time)\n",
    "            prev_str = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "            #prev_1_str = getattr(row[1], \"%s_Prev1\" % (log.time))\n",
    "            event_time = time.strptime(event_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "\n",
    "            if prev_str != 0:\n",
    "                prev_time = time.strptime(prev_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                diff = diff_prev_event.total_seconds()\n",
    "                event_day = prev_time.tm_wday\n",
    "\n",
    "            else: \n",
    "                diff = 0\n",
    "                event_day = None\n",
    "\n",
    "                        \n",
    "            if event_day != None:\n",
    "                if event_day == last_event_day:\n",
    "                    events_this_day += 1\n",
    "                else:\n",
    "                    last_event_day = event_day\n",
    "                    events_this_day = 1\n",
    "            else: \n",
    "                pass\n",
    "    \n",
    "            y_a[j, act] = 1\n",
    "            y_t[j] = diff            \n",
    "\n",
    "            k = 0\n",
    "            \n",
    "            for i in range(log.k -1, -1, -1):\n",
    "                \n",
    "                if getattr(row[1], \"%s_Prev%i\" % (log.activity, i)) != 0: # 0 indicates no activity (first activity is encoded to 1)\n",
    "                    X[j, log.k - i - 1, getattr(row[1], \"%s_Prev%i\" % (log.activity, i))] = 1\n",
    "                X[j, log.k - i - 1, len(activities)+2] = k\n",
    "                X[j, log.k - i - 1, len(activities) + 3] = time_diff # Diff in seconds\n",
    "\n",
    " \n",
    "                str_time = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "                if str_time != 0:\n",
    "                    event_time = time.strptime(str_time, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 5] = event_time.tm_wday  # Day of the week\n",
    "                else: \n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = 0 # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 5] = 0  # Day of the week\n",
    "                \n",
    "                X[j, log.k - 1 - 1, len(activities) + 6] = events_this_day\n",
    "\n",
    "    \n",
    "                try:\n",
    "                    prev_str = getattr(row[1], \"%s_Prev1\" % (log.time))\n",
    "                    #print(\"First success!\", prev_str)\n",
    "                    if prev_str != 0:\n",
    "\n",
    "                        prev_time = time.strptime(prev_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                        diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                        time_diff = diff_prev_event.total_seconds()\n",
    "                        #print(time_diff)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                        \n",
    "                    X[j, log.k - i - 1, len(activities) + 3] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_wday  # Day of the week\n",
    "\n",
    "                k += 1\n",
    "\n",
    "            j += 1\n",
    "\n",
    "    return X, y_a, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(log, epochs=4, early_stop=42):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "    from tensorflow.keras.layers import Input\n",
    "    from tensorflow.keras.layers import Dense, BatchNormalization, LSTM\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "    print(\"Transforming log...\")\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "\n",
    "    # build the model:\n",
    "    print('Build model...')\n",
    "    main_input = Input(shape=(log.k, len(np.unique(log.data[log.activity]))+7), name='main_input')\n",
    "    # train a 2-layer LSTM with one shared layer\n",
    "    l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer\n",
    "    b1 = BatchNormalization()(l1)\n",
    "    l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction\n",
    "    b2_1 = BatchNormalization()(l2_1)\n",
    "    l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in time prediction\n",
    "    b2_2 = BatchNormalization()(l2_2)\n",
    "\n",
    "    act_output = Dense(len(np.unique(log.data[log.activity])) + 1, activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\n",
    "    time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "    opt = Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "    model.compile(loss={'act_output':'categorical_crossentropy', 'time_output': 'mae'}, optimizer=opt)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop)\n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(\"model\", 'model_{epoch:03d}-{val_loss:.2f}.h5'), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    if len(y_a) > 10:\n",
    "        split = 0.2\n",
    "    else:\n",
    "        split = 0\n",
    "\n",
    "    model.fit(X, {'act_output': y_a, 'time_output': y_t}, validation_split=split, verbose=2, callbacks=[early_stopping, lr_reducer], batch_size=log.k, epochs=epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, log):\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "    pred_act, pred_time = model.predict(X)\n",
    "    predict_vals = np.argmax(pred_act, axis=1)\n",
    "    pred_time = pred_time.reshape(-1)\n",
    "    #predict_probs = predictions[np.arange(predictions.shape[0]), predict_vals]\n",
    "    expected_vals = np.argmax(y_a, axis=1)\n",
    "    #expected_probs = predictions[np.arange(predictions.shape[0]), expected_vals]\n",
    "    activity_acc = np.mean(expected_vals ==  predict_vals)\n",
    "    mae_time = np.mean(abs(y_t - pred_time)) / 86400\n",
    "    return predict_vals, pred_time, activity_acc, mae_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create k-context: 2\n",
      "Create k-context: 2\n",
      "Created k context\n"
     ]
    }
   ],
   "source": [
    "LSTM_log_train = LogFile(path_train, \",\", 0, None, 'event time:timestamp', case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=2)\n",
    "LSTM_log_test = LogFile(path_test, \",\", 0, None, 'event time:timestamp', case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=2)\n",
    "\n",
    "LSTM_map_train = LSTM_log_train.int_convert()\n",
    "LSTM_map_test = LSTM_log_test.int_convert()\n",
    "\n",
    "LSTM_log_train.remove_attributes(['eventID', 'case REG_DATE', 'case AMOUNT_REQ', 'event lifecycle:transition'])\n",
    "LSTM_log_test.remove_attributes(['eventID', 'case REG_DATE', 'case AMOUNT_REQ', 'event lifecycle:transition'])\n",
    "\n",
    "LSTM_log_train.create_k_context()\n",
    "LSTM_log_test.create_k_context()\n",
    "\n",
    "\n",
    "print(\"Created k context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming log...\n",
      "Build model...\n",
      "Epoch 1/2\n",
      "3326/3326 - 23s - loss: 34734.7461 - act_output_loss: 2.5549 - time_output_loss: 34732.1211 - val_loss: 40378.0508 - val_act_output_loss: 2.2559 - val_time_output_loss: 40375.8125 - lr: 0.0020 - 23s/epoch - 7ms/step\n",
      "Epoch 2/2\n",
      "3326/3326 - 13s - loss: 34729.1484 - act_output_loss: 2.3778 - time_output_loss: 34726.7578 - val_loss: 40379.9219 - val_act_output_loss: 2.2634 - val_time_output_loss: 40377.6992 - lr: 0.0020 - 13s/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "model = train_LSTM(LSTM_log_test, epochs=2, early_stop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_act, pred_time, acc_act, mae_time = test(model, LSTM_log_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_SUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_SUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14660</th>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14661</th>\n",
       "      <td>W_Nabellen incomplete dossiers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14662</th>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14663</th>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14664</th>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14665 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "0                         A_SUBMITTED\n",
       "1                   A_PARTLYSUBMITTED\n",
       "2                   A_PARTLYSUBMITTED\n",
       "3                         A_SUBMITTED\n",
       "4                   A_PARTLYSUBMITTED\n",
       "...                               ...\n",
       "14660          W_Completeren aanvraag\n",
       "14661  W_Nabellen incomplete dossiers\n",
       "14662          W_Completeren aanvraag\n",
       "14663          W_Completeren aanvraag\n",
       "14664          W_Completeren aanvraag\n",
       "\n",
       "[14665 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred_act).replace(LSTM_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\20204502\\OneDrive - TU Eindhoven\\Documents\\GitHub\\Process-Mining\\Process-Mining\\Deliv_Notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000047?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000047?line=2'>3</a>\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(true_val, predi)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000047?line=4'>5</a>\u001b[0m \u001b[39m# Making accuracy table\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000047?line=5'>6</a>\u001b[0m metrics_dict \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mclassification_report(true_val, predi, digits\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, output_dict \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'true_val' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_val, predi)\n",
    "\n",
    "# Making accuracy table\n",
    "metrics_dict = metrics.classification_report(true_val, predi, digits=6, output_dict = True)\n",
    "df2 = pd.DataFrame.from_dict(metrics_dict)\n",
    "df2\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt = 'g')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\");"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fafe8da2469ce22b82e1babda22c546ed40097b8f4cbd3e26d6446f22e5d370"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
